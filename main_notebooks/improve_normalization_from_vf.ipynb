{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performances of different post-merging normalization methods\n",
    "\n",
    "This notebook contains the implementation to compare different post-merging normalization methods in Section 4.3 of our paper.\n",
    "\n",
    "For ZipIt! experiments, please first run `PFM/get_zipit_permuted_model.ipynb` to obtain the permuted models and move them to the corresponding directories in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utils.utils import load_model\n",
    "from source.utils.connect import interpolate_state_dicts, eval_line\n",
    "from source.utils.weight_matching import weight_matching\n",
    "from source.utils.data_funcs import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from source.utils.train import validate\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from source.utils.logger import Logger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.setup_logging()\n",
    "logger = Logger()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class config:\n",
    "    model = 'cifar_vgg16'\n",
    "    dataset = 'cifar10'\n",
    "    print_freq = 100\n",
    "    path = '../data' # path to dataset\n",
    "    special_init = 'vgg_init'\n",
    "\n",
    "trainset, testset = load_data(config.path, config.dataset)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_alg = 'zipit' # 'wm' or 'zipit'\n",
    "n = 11 # number of merged models to evaluate\n",
    "config.n = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different normalization strategies on top of the WM-based merging and ZipIt!-based merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matching_alg == 'wm':\n",
    "    sd_1 = torch.load('../ex_results/same_init_ex/cifar10/cifar_vgg16/diff_init/seed_20/model_1_160.pt', map_location=device)\n",
    "    sd_2 = torch.load('../ex_results/same_init_ex/cifar10/cifar_vgg16/diff_init/seed_20/model_2_160.pt', map_location=device)\n",
    "    model_1 = load_model(config, 3).to(device)\n",
    "    model_2 = load_model(config, 3).to(device)\n",
    "    model_1.load_state_dict(sd_1)\n",
    "    model_2.load_state_dict(sd_2)\n",
    "    \n",
    "    # 1. REPAIR/RESCALE/Bias Calibration after Merging\n",
    "    stats_dict = {}\n",
    "    sd_list = []\n",
    "\n",
    "    for i in range(5):\n",
    "        cur_res_list = {}\n",
    "        sd_2_wm, _ = weight_matching(config.model, sd_1, sd_2, device=device)\n",
    "        sd_list.append(sd_2_wm)\n",
    "        model_2_wm = load_model(config, 3).to(device)\n",
    "        model_2_wm.load_state_dict(sd_2_wm)\n",
    "\n",
    "        wm_stats = eval_line(model_1, model_2_wm, testloader, criterion, device, config, n=n, repair=None, name=config.model)\n",
    "        repair_stats = eval_line(model_1, model_2_wm, testloader, criterion, device, config, n=n, repair='repair', bn_loader=trainloader, name=config.model)\n",
    "        rescale_stats = eval_line(model_1, model_2_wm, testloader, criterion, device, config, n=n, repair='rescale', bn_loader=trainloader , name=config.model)\n",
    "        bias_cal = eval_line(model_1, model_2_wm, testloader, criterion, device, config, n=n, repair=None, name=config.model, bias_norm=True)\n",
    "        cur_res_list['wm'] = wm_stats\n",
    "        cur_res_list['repair'] = repair_stats\n",
    "        cur_res_list['rescale'] = rescale_stats\n",
    "        cur_res_list['bias_cal'] = bias_cal\n",
    "        stats_dict[i] = cur_res_list\n",
    "\n",
    "    # 2. Bias Removal before Merging\n",
    "    sd_wm_mid = interpolate_state_dicts(sd_1, sd_2_wm, 0.5)\n",
    "    sd_ori = sd_wm_mid\n",
    "    sd_removebias = deepcopy(sd_ori)\n",
    "\n",
    "    keys = list(sd_1.keys())\n",
    "    bias_keys = []\n",
    "    for k in keys:\n",
    "        if 'bias' in k:\n",
    "            bias_keys.append(k)\n",
    "    bias_keys = np.array(bias_keys)\n",
    "\n",
    "    forward_test_acc_s = []\n",
    "    forward_test_loss_s = []\n",
    "\n",
    "    model_removebias = deepcopy(model_1)\n",
    "    for i in range(len(bias_keys)+1):\n",
    "        sd_removebias = deepcopy(sd_ori)\n",
    "        for k in bias_keys[:i]:\n",
    "            if 'bias' in k:\n",
    "                sd_removebias[k].fill_(0)\n",
    "        model_removebias.load_state_dict(sd_removebias)\n",
    "        loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "        forward_test_acc_s.append(acc)\n",
    "        forward_test_loss_s.append(loss)\n",
    "\n",
    "    backward_test_acc_s = [forward_test_acc_s[0]]\n",
    "    backward_test_loss_s = [forward_test_loss_s[0]]\n",
    "    for i in range(1, len(bias_keys)+1):\n",
    "        sd_removebias = deepcopy(sd_ori)\n",
    "        for k in bias_keys[-i:]:\n",
    "            if 'bias' in k:\n",
    "                sd_removebias[k].fill_(0)\n",
    "        model_removebias.load_state_dict(sd_removebias)\n",
    "        loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "        backward_test_acc_s.append(acc)\n",
    "        backward_test_loss_s.append(loss)\n",
    "        \n",
    "    plt.plot(forward_test_acc_s, label='forward')\n",
    "    plt.plot(backward_test_acc_s, label='backward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # choose the best result\n",
    "    backward_test_acc_s = np.array(backward_test_acc_s)\n",
    "    print(backward_test_acc_s)\n",
    "    remove_last_k_bias = np.argmax(backward_test_acc_s)\n",
    "    print(f'remove last {remove_last_k_bias} bias')\n",
    "    alpha_s = np.linspace(0.0, 1.0, n)\n",
    "    # alpha_s = [0.5]  # test \n",
    "    remove_bias_stasts_dict = {}\n",
    "    for i in range(5):\n",
    "        cur_res_list = {'bias_removal': {'loss': [], 'acc': []}}\n",
    "        for alpha in alpha_s:\n",
    "            sd_merged = interpolate_state_dicts(sd_1, sd_2_wm, alpha)\n",
    "            sd_removebias = deepcopy(sd_merged)\n",
    "            # remove bias\n",
    "            for k in bias_keys[-remove_last_k_bias:]:\n",
    "                if 'bias' in k:\n",
    "                    sd_removebias[k].fill_(0)\n",
    "            model_removebias.load_state_dict(sd_removebias)\n",
    "            loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "            cur_res_list['bias_removal']['loss'].append(loss)\n",
    "            cur_res_list['bias_removal']['acc'].append(acc)\n",
    "        remove_bias_stasts_dict[i] = cur_res_list\n",
    "\n",
    "    for i in remove_bias_stasts_dict:\n",
    "        remove_bias_stasts_dict[i]['bias_removal']['loss'] = np.array(remove_bias_stasts_dict[i]['bias_removal']['loss'])\n",
    "        remove_bias_stasts_dict[i]['bias_removal']['acc'] = np.array(remove_bias_stasts_dict[i]['bias_removal']['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZipIt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matching_alg == 'zipit':\n",
    "    stats_dict = {}\n",
    "    sd_list = []\n",
    "    for i in range(5):\n",
    "        # 1. REPAIR/RESCALE/Bias Calibration after Merging\n",
    "        sd_1 = torch.load(f'../PFM/checkpoints/cifar10_my_vgg16_{2*i+1}_zipit.pth')\n",
    "        sd_2 = torch.load(f'../PFM/checkpoints/cifar10_my_vgg16_{2*(i+1)}_zipit.pth')\n",
    "        model_1 = load_model(config, 3).to(device)\n",
    "        model_2 = load_model(config, 3).to(device)\n",
    "        model_1.load_state_dict(sd_1)\n",
    "        model_2.load_state_dict(sd_2)\n",
    "\n",
    "        cur_res_list = {}\n",
    "        merging_stats = eval_line(model_1, model_2, testloader, criterion, device, config, n=n, repair=None, name=config.model)\n",
    "        repair_stats = eval_line(model_1, model_2, testloader, criterion, device, config, n=n, repair='repair', bn_loader=trainloader, name=config.model)\n",
    "        rescale_stats = eval_line(model_1, model_2, testloader, criterion, device, config, n=n, repair='rescale', bn_loader=trainloader , name=config.model)\n",
    "        bias_cal = eval_line(model_1, model_2, testloader, criterion, device, config, n=n, repair=None, name=config.model, bias_norm=True)\n",
    "        cur_res_list['zipit'] = merging_stats\n",
    "        cur_res_list['repair'] = repair_stats\n",
    "        cur_res_list['rescale'] = rescale_stats\n",
    "        cur_res_list['bias_cal'] = bias_cal\n",
    "        stats_dict[i] = cur_res_list\n",
    "\n",
    "        # 2. Bias Removal before Merging\n",
    "        sd_wm_mid = interpolate_state_dicts(sd_1, sd_2, 0.5)\n",
    "        sd_ori = sd_wm_mid\n",
    "        sd_removebias = deepcopy(sd_ori)\n",
    "\n",
    "        keys = list(sd_1.keys())\n",
    "        bias_keys = []\n",
    "        for k in keys:\n",
    "            if 'bias' in k:\n",
    "                bias_keys.append(k)\n",
    "        bias_keys = np.array(bias_keys)\n",
    "\n",
    "        forward_test_acc_s = []\n",
    "        forward_test_loss_s = []\n",
    "\n",
    "        model_removebias = deepcopy(model_1)\n",
    "        for i in range(len(bias_keys)+1):\n",
    "            sd_removebias = deepcopy(sd_ori)\n",
    "            for k in bias_keys[:i]:\n",
    "                if 'bias' in k:\n",
    "                    sd_removebias[k].fill_(0)\n",
    "            model_removebias.load_state_dict(sd_removebias)\n",
    "            loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "            forward_test_acc_s.append(acc)\n",
    "            forward_test_loss_s.append(loss)\n",
    "\n",
    "        backward_test_acc_s = [forward_test_acc_s[0]]\n",
    "        backward_test_loss_s = [forward_test_loss_s[0]]\n",
    "        for i in range(1, len(bias_keys)+1):\n",
    "            sd_removebias = deepcopy(sd_ori)\n",
    "            for k in bias_keys[-i:]:\n",
    "                if 'bias' in k:\n",
    "                    sd_removebias[k].fill_(0)\n",
    "            model_removebias.load_state_dict(sd_removebias)\n",
    "            loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "            backward_test_acc_s.append(acc)\n",
    "            backward_test_loss_s.append(loss)\n",
    "\n",
    "        plt.plot(forward_test_acc_s, label='forward')\n",
    "        plt.plot(backward_test_acc_s, label='backward')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # choose the best result\n",
    "        backward_test_acc_s = np.array(backward_test_acc_s)\n",
    "        print(backward_test_acc_s)\n",
    "        remove_last_k_bias = np.argmax(backward_test_acc_s)\n",
    "        print(f'remove last {remove_last_k_bias} bias')\n",
    "        alpha_s = np.linspace(0.0, 1.0, n)\n",
    "        remove_bias_stasts_dict = {}\n",
    "        for i in range(5):\n",
    "            for i in range(5):\n",
    "                sd_1 = torch.load(f'../PFM/checkpoints/cifar10_my_vgg16_{2*i+1}_zipit.pth')\n",
    "                sd_2 = torch.load(f'../PFM/checkpoints/cifar10_my_vgg16_{2*(i+1)}_zipit.pth')\n",
    "                model_removebias = load_model(config, 3).to(device)\n",
    "\n",
    "                cur_res_list = {'bias_removal': {'loss': [], 'acc': []}}\n",
    "                for alpha in alpha_s:\n",
    "                    sd_merged = interpolate_state_dicts(sd_1, sd_2, alpha)\n",
    "                    sd_removebias = deepcopy(sd_merged)\n",
    "                    # remove bias\n",
    "                    for k in bias_keys[-remove_last_k_bias:]:\n",
    "                        if 'bias' in k:\n",
    "                            sd_removebias[k].fill_(0)\n",
    "                    model_removebias.load_state_dict(sd_removebias)\n",
    "                    loss, acc, _, _ = validate(testloader, model_removebias, criterion, device, config)\n",
    "                    cur_res_list['bias_removal']['loss'].append(loss)\n",
    "                    cur_res_list['bias_removal']['acc'].append(acc)\n",
    "                remove_bias_stasts_dict[i] = cur_res_list\n",
    "\n",
    "        for i in remove_bias_stasts_dict:\n",
    "            remove_bias_stasts_dict[i]['bias_removal']['loss'] = np.array(remove_bias_stasts_dict[i]['bias_removal']['loss'])\n",
    "            remove_bias_stasts_dict[i]['bias_removal']['acc'] = np.array(remove_bias_stasts_dict[i]['bias_removal']['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = {}\n",
    "for i in stats_dict:\n",
    "    summary_dict.update({i: {}})\n",
    "    for method in stats_dict[i]:\n",
    "        summary_dict[i][method] = {}\n",
    "        summary_dict[i][method]['loss'] = stats_dict[i][method][:, 0].numpy()\n",
    "        summary_dict[i][method]['acc'] = stats_dict[i][method][:, 1].numpy()\n",
    "\n",
    "for i in summary_dict:\n",
    "    summary_dict[i].update(remove_bias_stasts_dict[i])\n",
    "# calculate the mean and std across 5 experiments\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "for method in summary_dict[0]:\n",
    "    mean_dict[method] = {}\n",
    "    std_dict[method] = {}\n",
    "    for metric in summary_dict[0][method]:\n",
    "        mean_dict[method][metric] = np.mean([summary_dict[i][method][metric] for i in summary_dict], axis=0)\n",
    "        std_dict[method][metric] = np.std([summary_dict[i][method][metric] for i in summary_dict], axis=0)\n",
    "summary_dict['mean'] = mean_dict\n",
    "summary_dict['std'] = std_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('CPAL_plots', exist_ok=True)\n",
    "save_name = f'CPAL_plots/renorm_{matching_alg}_cifar10_vgg16.pth'\n",
    "if os.path.exists(save_name):\n",
    "    raise ValueError('file exists')\n",
    "else:\n",
    "    torch.save(summary_dict, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "summary_dict = torch.load(f'CPAL_plots/renorm_{matching_alg}_cifar10_vgg16.pth')\n",
    "mean_dict = summary_dict['mean']\n",
    "std_dict = summary_dict['std']\n",
    "\n",
    "methods = ['wm', 'zipit', 'repair', 'rescale', 'bias_cal', 'bias_removal']\n",
    "methods_label_map = {'wm': 'WM', 'zipit': 'ZipIt!', 'repair': f'{matching_alg} + Repair', 'rescale': f'{matching_alg} + Rescale', 'bias_cal': f'{matching_alg} + Bias Cal.', 'bias_removal': f'{matching_alg} + Bias Removal'}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "x_s = np.linspace(0, 1.1, n)\n",
    "\n",
    "for method in methods:\n",
    "    if method in mean_dict:\n",
    "        plt.errorbar(x_s, mean_dict[method]['acc'], yerr=std_dict[method]['acc'], label=methods_label_map[method], capsize=5)\n",
    "\n",
    "plt.xlabel(r'Merging Coefficient ($\\alpha$)', fontsize=20)\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=20)\n",
    "plt.yticks([0, 20, 40, 60, 80, 90])\n",
    "plt.ylim(0, 95)\n",
    "plt.title('Compare Post-Merging Normalization Strategy', fontsize=20)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
